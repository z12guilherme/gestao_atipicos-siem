# Este é o pipeline principal do Logstash.
# Ele lê logs de um arquivo, os processa e os envia para o Elasticsearch.

input {
  file {
    path => "/usr/share/logstash/data/sample_logs.ndjson" # Caminho dentro do container
    start_position => "beginning"
    sincedb_path => "/dev/null" # Para reler o arquivo a cada reinicialização do Logstash
    codec => "json"
  }
}

filter {
  # Adiciona geolocalização baseada no IP do cliente
  geoip {
    source => "[proxy][clientIp]"
    target => "geoip"
  }

  # Converte o timestamp da Vercel (em milissegundos) para o formato correto
  date {
    match => [ "timestamp", "UNIX_MS" ]
  }

  # Verifica se o IP do cliente está na nossa lista de ameaças
  translate {
    field => "[proxy][clientIp]"
    destination => "[threat][match]"
    dictionary_path => "/usr/share/logstash/data/blocklist.csv" # Caminho dentro do container
    fallback => "false"
  }
}

output {
  # Envia para o Elasticsearch que está rodando no mesmo Docker Compose
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "gestao_atipicos_logs-%{+YYYY.MM.dd}"
  }

  # Mostra no terminal também para debug
  stdout {
    codec => rubydebug
  }
}
